import os
import sys
import threading
import time
import json
import queue
import traceback # For detailed error logging

import pyaudio
import numpy as np
import whisper
import ollama
from flask import Flask, request, jsonify
from flask_socketio import SocketIO, emit

# --- Configuration ---
AUDIO_CHUNK_SIZE = 1024  # Size of audio buffer chunks
AUDIO_FORMAT = pyaudio.paInt16
AUDIO_CHANNELS = 1
AUDIO_RATE = 16000  # Sample rate for Whisper
WHISPER_MODEL = "small.en"  # "tiny.en", "base.en", "small.en", "medium.en" - choose based on performance/accuracy
OLLAMA_MODEL = "phi-3:latest" # Or "phi4:latest"

# --- Global Variables ---
app = Flask(__name__)
# Use a more explicit CORS configuration for development
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading') # Adjust cors_allowed_origins for production
audio_stream = None
is_listening = False
audio_queue = queue.Queue()
transcript_queue = queue.Queue()
cheat_sheet_data = {} # Stores extracted entities: {name: {type, description}}
ollama_prompt_template = """
You are an AI assistant specialized in extracting named entities from video transcripts to create a structured cheat sheet.
Your task is to identify and describe:
- Characters: Individuals, animals, or sentient beings, their role, and key relationships.
- Locations: Specific places (cities, buildings, fictional realms), their significance.
- Organizations: Groups, factions, institutions.
- Key Objects/Items: Important artifacts, tools, or unique items mentioned.
- Concepts/Events: Important ideas, theories, historical events, or major plot points mentioned.

For each identified entity, provide its 'type', 'name', and a concise 'description'.
If an entity has been previously mentioned in the context and new information is provided, *update its 'description'*.
Only include entities that are clearly named or explicitly described in the transcript.
Always output a JSON array of objects. If no *new or updated* entities are found, return an empty array `[]`.

Example JSON for a Character: {"type": "Character", "name": "John Doe", "description": "A brave knight who served King Arthur."}
Example JSON for a Location: {"type": "Location", "name": "Camelot", "description": "King Arthur's legendary castle."}

Current cheat sheet context (as JSON array):
{}

Transcript chunk:
{}

JSON Output:
"""

# --- Audio Functions ---
def find_virtual_cable_input_device():
    p = pyaudio.PyAudio()
    info = p.get_host_api_info_by_index(0)
    num_devices = info.get('deviceCount')
    for i in range(0, num_devices):
        if (p.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:
            device_name = p.get_device_info_by_host_api_device_index(0, i).get('name')
            print(f"Checking device: {device_name}")
            if "CABLE Output (VB-Audio Virtual Cable)" in device_name: # This is the virtual microphone
                print(f"Found virtual cable input device: {device_name} (ID: {i})")
                p.terminate()
                return i
            if "Stereo Mix" in device_name and sys.platform == "win32": # Fallback for some systems
                print(f"Found Stereo Mix input device (fallback): {device_name} (ID: {i})")
                p.terminate()
                return i
    p.terminate()
    print("WARNING: VB-Audio Virtual Cable Input Device not found. Please ensure it's installed and configured correctly.")
    return -1

def audio_recorder():
    global audio_stream, is_listening
    p = pyaudio.PyAudio()
    device_id = find_virtual_cable_input_device()

    if device_id == -1:
        socketio.emit('status', {'message': 'Error: Virtual Audio Cable not found. Please install and configure it.'})
        return

    try:
        audio_stream = p.open(format=AUDIO_FORMAT,
                              channels=AUDIO_CHANNELS,
                              rate=AUDIO_RATE,
                              input=True,
                              frames_per_buffer=AUDIO_CHUNK_SIZE,
                              input_device_index=device_id)
        socketio.emit('status', {'message': 'Listening to audio...'})
        print("Audio stream started.")

        # Accumulate audio for Whisper. Whisper performs better on larger chunks (e.g., 5-15 seconds)
        audio_buffer = []
        buffer_duration = 5 # seconds
        frames_per_buffer = int(AUDIO_RATE * buffer_duration / (AUDIO_CHUNK_SIZE * 0.5)) # Adjust for overlap

        while is_listening:
            try:
                data = audio_stream.read(AUDIO_CHUNK_SIZE, exception_on_overflow=False)
                audio_buffer.append(data)
                # Process buffer when it reaches desired duration
                if len(audio_buffer) >= frames_per_buffer:
                    full_audio_data = b''.join(audio_buffer)
                    audio_queue.put(full_audio_data)
                    # Keep some overlap for better context (e.g., last 2 seconds)
                    overlap_frames = int(AUDIO_RATE * 2 / AUDIO_CHUNK_SIZE)
                    audio_buffer = audio_buffer[-overlap_frames:] # Keep last 'overlap_frames' for next buffer
            except IOError as e:
                # Handle audio device issues, e.g., unplugged headset
                print(f"Audio stream error: {e}")
                socketio.emit('status', {'message': f'Audio error: {e}. Stopping listening.'})
                is_listening = False
                break
            time.sleep(0.01) # Small delay to prevent busy-waiting

    except Exception as e:
        print(f"Failed to open audio stream: {e}")
        socketio.emit('status', {'message': f'Error opening audio stream: {e}'})
        is_listening = False
    finally:
        if audio_stream:
            audio_stream.stop_stream()
            audio_stream.close()
            print("Audio stream stopped.")
        p.terminate()
        socketio.emit('status', {'message': 'Audio capture stopped.'})

# --- Whisper Transcription ---
def transcribe_audio():
    global is_listening
    try:
        model = whisper.load_model(WHISPER_MODEL)
        print(f"Whisper model '{WHISPER_MODEL}' loaded.")
        socketio.emit('status', {'message': f'Whisper model loaded: {WHISPER_MODEL}'})
    except Exception as e:
        print(f"Failed to load Whisper model: {e}")
        socketio.emit('status', {'message': f'Error loading Whisper model: {e}'})
        is_listening = False # Stop if model can't load
        return

    while is_listening or not audio_queue.empty():
        if not audio_queue.empty():
            audio_data = audio_queue.get()
            audio_np = np.frombuffer(audio_data, dtype=np.int16).flatten().astype(np.float32) / 32768.0
            
            try:
                result = model.transcribe(audio_np, fp16=False) # Set fp16=False if no GPU
                transcript = result["text"].strip()
                if transcript: # Only process non-empty transcripts
                    print(f"Transcript: {transcript}")
                    transcript_queue.put(transcript)
                    socketio.emit('new_transcript', {'text': transcript}) # Send live transcript to UI
            except Exception as e:
                print(f"Error during transcription: {e}")
                traceback.print_exc() # Print full traceback
        else:
            time.sleep(0.1) # Wait if queue is empty

    print("Transcription thread stopped.")

# --- Ollama LLM Processing ---
def process_transcript_with_ollama():
    global cheat_sheet_data, is_listening
    while is_listening or not transcript_queue.empty():
        if not transcript_queue.empty():
            transcript = transcript_queue.get()
            
            # Prepare current cheat sheet for LLM context
            current_cheat_sheet_json = json.dumps(list(cheat_sheet_data.values()), indent=2)
            
            # Construct LLM prompt
            prompt = ollama_prompt_template.format(current_cheat_sheet_json, transcript)
            
            messages = [
                {"role": "user", "content": prompt}
            ]

            try:
                # print("Sending to Ollama...")
                response = ollama.chat(model=OLLAMA_MODEL, messages=messages, format='json')
                content = response['message']['content']
                # print(f"Ollama Raw Response: {content}") # Debug: See raw LLM output

                try:
                    extracted_entities = json.loads(content)
                    if isinstance(extracted_entities, list):
                        for entity in extracted_entities:
                            if 'name' in entity and 'type' in entity and 'description' in entity:
                                # Update existing or add new
                                existing_entity = cheat_sheet_data.get(entity['name'])
                                if existing_entity:
                                    # If description is more detailed, update it
                                    if len(entity['description']) > len(existing_entity['description']):
                                        cheat_sheet_data[entity['name']].update(entity)
                                    # Ensure type is consistent, or handle conflicts
                                else:
                                    cheat_sheet_data[entity['name']] = entity
                                
                                # Emit to frontend
                                socketio.emit('update_cheat_sheet', cheat_sheet_data[entity['name']])
                            else:
                                print(f"Malformed entity: {entity}")
                    else:
                        print(f"Ollama did not return a JSON array: {content}")

                except json.JSONDecodeError as e:
                    print(f"Failed to decode Ollama JSON: {e}")
                    print(f"Content that caused error: {content}")
                
            except Exception as e:
                print(f"Error calling Ollama: {e}")
                traceback.print_exc()
        else:
            time.sleep(0.1)

    print("Ollama processing thread stopped.")

# --- Flask API Endpoints ---
@app.route('/start', methods=['POST'])
def start_processing():
    global is_listening
    if not is_listening:
        is_listening = True
        socketio.emit('status', {'message': 'Starting...'})
        # Start threads
        threading.Thread(target=audio_recorder).start()
        threading.Thread(target=transcribe_audio).start()
        threading.Thread(target=process_transcript_with_ollama).start()
        return jsonify({"status": "started"}), 200
    return jsonify({"status": "already running"}), 200

@app.route('/stop', methods=['POST'])
def stop_processing():
    global is_listening
    if is_listening:
        is_listening = False
        if audio_stream:
            audio_stream.stop_stream() # Ensure stream is explicitly stopped
        socketio.emit('status', {'message': 'Stopping...'})
        # Clear queues
        while not audio_queue.empty(): audio_queue.get()
        while not transcript_queue.empty(): transcript_queue.get()
        print("Backend stopping...")
        return jsonify({"status": "stopping"}), 200
    return jsonify({"status": "not running"}), 200

@app.route('/status', methods=['GET'])
def get_status():
    return jsonify({"is_listening": is_listening, "cheat_sheet_size": len(cheat_sheet_data)}), 200

@app.route('/cheat_sheet', methods=['GET'])
def get_cheat_sheet():
    return jsonify(list(cheat_sheet_data.values())), 200

# --- SocketIO Events ---
@socketio.on('connect')
def test_connect():
    print("Client connected!")
    emit('status', {'message': 'Connected to backend.'})

@socketio.on('disconnect')
def test_disconnect():
    print("Client disconnected.")

# --- Main execution ---
if __name__ == '__main__':
    # When run directly, start the Flask/SocketIO server
    # Use allow_unsafe_werkzeug_shutdown=True for clean shutdown during development
    print("Starting Flask/SocketIO server...")
    socketio.run(app, host='127.0.0.1', port=5000, debug=False, allow_unsafe_werkzeug_shutdown=True)